{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsZWW1bbfpgvbWpgCEIuoN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bempong-Sylvester-Obese/Food-segmentation-with-pre-trained-model/blob/main/Food_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENVIRONEMNT SETUP**"
      ],
      "metadata": {
        "id": "nMFgaDr8G39i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Otoo_jxzsRcU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# Device and GPU setup\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Project Repository\n",
        "ROOT_CONTENT_DIR = \"/content\"\n",
        "\n",
        "# remove any existing /content/content directory\n",
        "NESTED_CONTENT_DIR = os.path.join(ROOT_CONTENT_DIR, \"content\")\n",
        "if os.path.exists(NESTED_CONTENT_DIR):\n",
        "    print(f\"Removing existing nested content directory: {NESTED_CONTENT_DIR}\")\n",
        "    shutil.rmtree(NESTED_CONTENT_DIR)\n",
        "\n",
        "\n",
        "PROJECT_DIR_NAME = \"food_segmentation\"\n",
        "PROJECT_DIR = os.path.join(ROOT_CONTENT_DIR, PROJECT_DIR_NAME)\n",
        "\n",
        "# Remove existing project directory\n",
        "if os.path.exists(PROJECT_DIR):\n",
        "    print(f\"Removing existing project directory: {PROJECT_DIR}\")\n",
        "    shutil.rmtree(PROJECT_DIR)\n",
        "\n",
        "# Create the project directory and change into it\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "os.chdir(PROJECT_DIR)\n",
        "ABS_PROJECT_DIR = os.getcwd()\n",
        "\n",
        "sys.path.append(ABS_PROJECT_DIR)\n",
        "\n",
        "print(\"Cloning model repositories..\")\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git GroundingDINO\n",
        "!git clone https://github.com/ChaoningZhang/MobileSAM.git MobileSAM\n",
        "\n",
        "# Dependency Installation\n",
        "print(\"Installing dependencies\")\n",
        "!pip install -q -e ./GroundingDINO\n",
        "!pip install -q -e ./MobileSAM\n",
        "\n",
        "# Model weight download\n",
        "print(\"Downloading model weights..\")\n",
        "\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth -P ./GroundingDINO\n",
        "!wget -q https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt -P ./MobileSAM\n",
        "\n",
        "print(\"\\nSetup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING MODEL"
      ],
      "metadata": {
        "id": "WOYC014KIu9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add GroundingDINO directory to sys.path\n",
        "grounding_dino_dir = os.path.join(ABS_PROJECT_DIR, \"GroundingDINO\")\n",
        "if grounding_dino_dir not in sys.path:\n",
        "    sys.path.append(grounding_dino_dir)\n",
        "    print(f\"Added {grounding_dino_dir} to sys.path\")\n",
        "\n",
        "# Add MobileSAM directory to sys.path\n",
        "mobile_sam_dir = os.path.join(ABS_PROJECT_DIR, \"MobileSAM\")\n",
        "if mobile_sam_dir not in sys.path:\n",
        "    sys.path.append(mobile_sam_dir)\n",
        "    print(f\"Added {mobile_sam_dir} to sys.path\")\n",
        "\n",
        "try:\n",
        "    from groundingdino.util.inference import Model as GroundingDINO\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"Error importing GroundingDINO model: {e}\")\n",
        "    sys.exit(\"Could not import GroundingDINO model.\")\n",
        "\n",
        "\n",
        "from mobile_sam import sam_model_registry, SamPredictor\n",
        "\n",
        "# GroundingDINO configuration\n",
        "GROUNDING_DINO_CONFIG_PATH = os.path.join(ABS_PROJECT_DIR, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(ABS_PROJECT_DIR, \"GroundingDINO/groundingdino_swint_ogc.pth\")\n",
        "\n",
        "# MobileSAM configuration\n",
        "MOBILE_SAM_CHECKPOINT_PATH = os.path.join(ABS_PROJECT_DIR, \"MobileSAM/mobile_sam.pt\")\n",
        "SAM_TYPE = \"vit_t\"\n",
        "\n",
        "print(\"Loading GroundingDINO model..\")\n",
        "grounding_dino_model = GroundingDINO(GROUNDING_DINO_CONFIG_PATH, GROUNDING_DINO_CHECKPOINT_PATH, DEVICE)\n",
        "\n",
        "print(\"Loading MobileSAM model..\")\n",
        "sam = sam_model_registry[SAM_TYPE](checkpoint=MOBILE_SAM_CHECKPOINT_PATH)\n",
        "sam.to(DEVICE)\n",
        "sam_predictor = SamPredictor(sam)\n",
        "\n",
        "print(\"\\nModels loaded successfully\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j7b1HQt_IdRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE DEFINITION AND TEXT PROMPTS"
      ],
      "metadata": {
        "id": "vpZXlNMS6RQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Image path\n",
        "IMAGE_NAME = \"/content/food_segmentation/Raw Images/5e1c4e8c-meal_image_31686_2023-08-04_111.jpg\"\n",
        "\n",
        "IMAGE_PATH = os.path.join(ABS_PROJECT_DIR, IMAGE_NAME)\n",
        "\n",
        "\n",
        "print(f\"Using image path from: {IMAGE_PATH}\")\n",
        "\n",
        "# Check if the image file exists\n",
        "if not os.path.exists(IMAGE_PATH):\n",
        "    print(f\"Error: Image file not found at {IMAGE_PATH}\")\n",
        "else:\n",
        "    print(f\"Image file found at {IMAGE_PATH}\")\n",
        "    # Text Prompt\n",
        "    CLASSES =[\"Yam\"]\n",
        "    PROMPT = \"A Sliced Yam\"\n",
        "\n",
        "    # Visualisation helper function\n",
        "    def show_image(images, titles=None, cols=2):\n",
        "        rows = math.ceil(len(images) / cols)\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "        axes = axes.flatten() if rows > 1 or cols > 1 else [axes]\n",
        "\n",
        "        for i, img in enumerate(images):\n",
        "            axes[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "            if titles and i < len(titles):\n",
        "                axes[i].set_title(titles[i])\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for j in range(i + 1, len(axes)):\n",
        "            axes[j].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    source_image = cv2.imread(IMAGE_PATH)\n",
        "    show_image([source_image], titles=[\"Source Image\"], cols=1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "50Kgpi266cXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INFERENCE PIPELINE"
      ],
      "metadata": {
        "id": "7VUrHd4HJAcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import supervision as sv\n",
        "\n",
        "# Detect with GroundingDINO\n",
        "print(\"Running GroundingDINO detection..\")\n",
        "detections, _, = grounding_dino_model.predict_with_caption(\n",
        "    image=source_image,\n",
        "    caption=PROMPT,\n",
        "    box_threshold=0.35,\n",
        "    text_threshold=0.25\n",
        ")\n",
        "print(f\"Detected {len(detections)} objects.\")\n",
        "\n",
        "if len(detections) > 0:\n",
        "    detections.class_id = np.zeros(len(detections), dtype=int)\n",
        "    if detections.confidence is None:\n",
        "        detections.confidence = np.ones(len(detections))\n",
        "    else:\n",
        "        detections.confidence = np.array(detections.confidence)\n",
        "else:\n",
        "    detections.class_id = np.array([], dtype=int)\n",
        "    detections.confidence = np.array([])\n",
        "\n",
        "\n",
        "print(\"Running MobileSAM segmentation...\")\n",
        "\n",
        "# Set image for SAM predictor\n",
        "sam_predictor.set_image(cv2.cvtColor(source_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# GroundingDINO detections as input boxes for SAM\n",
        "if len(detections) > 0:\n",
        "    input_boxes = detections.xyxy\n",
        "else:\n",
        "    input_boxes = np.array([])\n",
        "\n",
        "masks = []\n",
        "\n",
        "# predict mask individually\n",
        "if len(input_boxes) > 0:\n",
        "    for box in input_boxes:\n",
        "        box = box.reshape(1, -1)\n",
        "        mask, scores, logits = sam_predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=box,\n",
        "            multimask_output=False, # one mask per bounding box\n",
        "        )\n",
        "        if isinstance(mask, torch.Tensor):\n",
        "            mask = mask.cpu().numpy()\n",
        "        masks.append(np.squeeze(mask))\n",
        "\n",
        "\n",
        "# Connect masks into a single array\n",
        "if len(masks) > 0:\n",
        "    masks = np.array(masks)\n",
        "    print(f\"Generated {masks.shape[0]} masks.\")\n",
        "else:\n",
        "    masks = np.array([])\n",
        "    print(\"No masks generated.\")\n",
        "\n",
        "\n",
        "# Link Detections and Masks\n",
        "if len(detections) > 0 and len(masks) == len(detections):\n",
        "    detections.mask = masks\n",
        "elif len(detections) > 0 and len(masks) != len(detections):\n",
        "     print(\"Warning: Mask count does not match detection count.\")\n",
        "     detections.mask = np.array([])\n",
        "else:\n",
        "    detections.mask = np.array([])\n",
        "    print(\"No detections or masks.\")\n",
        "\n",
        "\n",
        "# Filter any empty detections\n",
        "if detections.mask is not None and isinstance(detections.mask, np.ndarray) and detections.mask.shape[0] > 0:\n",
        "    detections = detections[detections.mask.sum(axis=(1, 2)) > 0]\n",
        "else:\n",
        "    detections = sv.Detections.empty()\n",
        "\n",
        "print(f\"Final number of detections with masks: {len(detections)}\")"
      ],
      "metadata": {
        "id": "D5xlxLZnJEtO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESULT VISUALIZATION"
      ],
      "metadata": {
        "id": "jedFptHgKsx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(color=sv.Color(b=0, g=0, r=255), thickness=2) # BGR for Red\n",
        "mask_annotator = sv.MaskAnnotator(color=sv.Color(b=255, g=0, r=0)) # BGR for Blue\n",
        "\n",
        "# Annotation with bounding boxes\n",
        "annotated_image_boxes = box_annotator.annotate(\n",
        "    scene=source_image.copy(),\n",
        "    detections=detections,\n",
        ")\n",
        "\n",
        "# Annotate with masks\n",
        "annotated_image_masks = mask_annotator.annotate(\n",
        "    scene=source_image.copy(),\n",
        "    detections=detections\n",
        ")\n",
        "\n",
        "# 3. Annotate with both boxes and masks\n",
        "combined_annotated_image = mask_annotator.annotate(\n",
        "    scene=source_image.copy(),\n",
        "    detections=detections\n",
        ")\n",
        "combined_annotated_image = box_annotator.annotate(\n",
        "    scene=combined_annotated_image,\n",
        "    detections=detections,\n",
        ")\n",
        "\n",
        "print(\"\\nDisplaying Results:\")\n",
        "\n",
        "# Create results directory if it doesn't exist\n",
        "results_dir = os.path.join(ABS_PROJECT_DIR, \"results\")\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Display the annotated images in a grid\n",
        "images_to_display = [\n",
        "    annotated_image_boxes,\n",
        "    annotated_image_masks,\n",
        "    combined_annotated_image\n",
        "]\n",
        "titles_for_display = [\n",
        "    \"GroundingDINO Bounding Boxes\",\n",
        "    \"MobileSAM Segmentation Masks\",\n",
        "    \"Final Combined Result\"\n",
        "]\n",
        "\n",
        "show_image(images_to_display, titles=titles_for_display, cols=3) # Display in 3 columns"
      ],
      "metadata": {
        "id": "69y70GfVKsH1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}